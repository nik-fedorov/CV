{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Машинный перевод"
      ],
      "metadata": {
        "id": "ZvjPiHcskGn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ноутбук содержит код трансформера, обучаемого на задачу машинного перевода с немецкого на английский язык"
      ],
      "metadata": {
        "id": "gvopH1DTkMBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu==2.3.1"
      ],
      "metadata": {
        "trusted": true,
        "id": "n8Z6KjVMkEQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "import math\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchtext\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "\n",
        "print('torch:', torch.__version__, \n",
        "      '\\ntorchtext:', torchtext.__version__, \n",
        "      '\\nwandb:', wandb.__version__)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-03-21T22:10:31.197887Z",
          "iopub.execute_input": "2023-03-21T22:10:31.198543Z",
          "iopub.status.idle": "2023-03-21T22:10:35.124842Z",
          "shell.execute_reply.started": "2023-03-21T22:10:31.198500Z",
          "shell.execute_reply": "2023-03-21T22:10:35.122815Z"
        },
        "trusted": true,
        "id": "aJA6FTQWkEQv",
        "outputId": "e3b09bef-e4ef-4712-f77b-18b12bb95e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "torch: 1.13.0+cpu \ntorchtext: 0.14.0 \nwandb: 0.13.10\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_EN_PATH = '/kaggle/input/bhw2-dataset/data/train.de-en.en'\n",
        "TRAIN_DE_PATH = '/kaggle/input/bhw2-dataset/data/train.de-en.de'\n",
        "VAL_EN_PATH = '/kaggle/input/bhw2-dataset/data/val.de-en.en'\n",
        "VAL_DE_PATH = '/kaggle/input/bhw2-dataset/data/val.de-en.de'\n",
        "TEST_EN_PATH = '/kaggle/working/test1.de-en.en'\n",
        "TEST_DE_PATH = '/kaggle/input/bhw2-dataset/data/test1.de-en.de'\n",
        "\n",
        "WORKING_DIR = '/kaggle/working/'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-21T16:51:57.459397Z",
          "iopub.execute_input": "2023-03-21T16:51:57.459831Z",
          "iopub.status.idle": "2023-03-21T16:51:57.467226Z",
          "shell.execute_reply.started": "2023-03-21T16:51:57.459787Z",
          "shell.execute_reply": "2023-03-21T16:51:57.466174Z"
        },
        "trusted": true,
        "id": "297Fe8RWkEQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-21T16:51:57.573082Z",
          "iopub.execute_input": "2023-03-21T16:51:57.573367Z",
          "iopub.status.idle": "2023-03-21T16:52:01.471409Z",
          "shell.execute_reply.started": "2023-03-21T16:51:57.573340Z",
          "shell.execute_reply": "2023-03-21T16:52:01.470006Z"
        },
        "trusted": true,
        "id": "cw-GBc8-kEQx",
        "outputId": "52cf4bdc-94a6-4d86-b9cc-5f40824bd22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_iterator(path):\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            yield line.strip().split()\n",
        "\n",
        "\n",
        "specials = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "vocab_en = build_vocab_from_iterator(\n",
        "    dataset_iterator(TRAIN_EN_PATH),\n",
        "    specials=specials, max_tokens=15000\n",
        ")\n",
        "\n",
        "vocab_de = build_vocab_from_iterator(\n",
        "    dataset_iterator(TRAIN_DE_PATH),\n",
        "    specials=specials, max_tokens=15000\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-21T16:52:01.476105Z",
          "iopub.execute_input": "2023-03-21T16:52:01.476448Z",
          "iopub.status.idle": "2023-03-21T16:52:04.161060Z",
          "shell.execute_reply.started": "2023-03-21T16:52:01.476414Z",
          "shell.execute_reply": "2023-03-21T16:52:04.160058Z"
        },
        "trusted": true,
        "id": "AFjmxETRkEQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_num_workers = 2\n",
        "batch_size = 128\n",
        "max_length = 128\n",
        "\n",
        "def texts_to_tensor(path, vocab):\n",
        "    tokenized_texts = []\n",
        "    for text in dataset_iterator(path):\n",
        "        tokens = [vocab[word] if word in vocab else vocab['<unk>'] for word in text]\n",
        "        tokens = [vocab['<bos>']] + tokens + [vocab['<eos>']]\n",
        "        tokenized_texts += [tokens]\n",
        "    \n",
        "    tensor = torch.full((len(tokenized_texts), max_length), vocab['<pad>'], dtype=torch.long)\n",
        "    for i, tokens in enumerate(tokenized_texts):\n",
        "        tensor[i, :len(tokens)] = torch.tensor(tokens)\n",
        "    \n",
        "    return tensor\n",
        "\n",
        "\n",
        "train_de = texts_to_tensor(TRAIN_DE_PATH, vocab_de)\n",
        "train_en = texts_to_tensor(TRAIN_EN_PATH, vocab_en)\n",
        "val_de = texts_to_tensor(VAL_DE_PATH, vocab_de)\n",
        "val_en = texts_to_tensor(VAL_EN_PATH, vocab_en)\n",
        "\n",
        "train_dataset = TensorDataset(train_de, train_en)\n",
        "test_dataset = TensorDataset(val_de, val_en)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
        "                          num_workers=dataloader_num_workers, pin_memory=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
        "                        num_workers=dataloader_num_workers, pin_memory=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-21T16:52:04.175874Z",
          "iopub.execute_input": "2023-03-21T16:52:04.176797Z",
          "iopub.status.idle": "2023-03-21T16:52:19.970544Z",
          "shell.execute_reply.started": "2023-03-21T16:52:04.176757Z",
          "shell.execute_reply": "2023-03-21T16:52:19.969538Z"
        },
        "trusted": true,
        "id": "o5iuthPUkEQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My tranformer"
      ],
      "metadata": {
        "id": "Ez-YoN2ckEQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    # Adapted from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "    def __init__(self, max_length, embed_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        positions = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
        "        freqs = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float) * \\\n",
        "                          (-math.log(10000) / embed_dim)).unsqueeze(0)\n",
        "\n",
        "        arguments = positions * freqs\n",
        "        pos_features = torch.zeros(max_length, embed_dim)\n",
        "        pos_features[:, 0::2] = torch.sin(arguments)\n",
        "        pos_features[:, 1::2] = torch.cos(arguments)\n",
        "        pos_features = pos_features.unsqueeze(0)\n",
        "        self.register_buffer('pos_features', pos_features)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # batched\n",
        "        pos_encodings = self.pos_features[:, :inputs.shape[1]]\n",
        "        outputs = inputs + pos_encodings\n",
        "        return self.dropout(outputs)\n",
        "\n",
        "\n",
        "class MyTransformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, max_length, embed_dim, fc_dim, \n",
        "                 num_heads, num_encoder_layers, num_decoder_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, embed_dim)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, embed_dim)\n",
        "        self.pos_encoder = PositionalEncoder(max_length, embed_dim, dropout)\n",
        "        self.transformer = nn.Transformer(d_model=embed_dim, nhead=num_heads, \n",
        "                                          num_encoder_layers=num_encoder_layers, \n",
        "                                          num_decoder_layers=num_decoder_layers, \n",
        "                                          dim_feedforward=fc_dim, dropout=dropout, \n",
        "                                          batch_first=True)\n",
        "        self.classifier = nn.Linear(embed_dim, tgt_vocab_size)\n",
        "    \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask, src_pad_mask=None, \n",
        "                tgt_pad_mask=None, memory_pad_mask=None):\n",
        "        src = self.src_embedding(src) * math.sqrt(self.embed_dim)\n",
        "        tgt = self.tgt_embedding(tgt) * math.sqrt(self.embed_dim)\n",
        "        \n",
        "        src = self.pos_encoder(src)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "        \n",
        "        out = self.transformer(src, tgt, \n",
        "                               tgt_mask=tgt_mask, src_mask=src_mask,\n",
        "                               src_key_padding_mask=src_pad_mask,\n",
        "                               tgt_key_padding_mask=tgt_pad_mask,\n",
        "                               memory_key_padding_mask=memory_pad_mask)\n",
        "        return self.classifier(out)\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz))) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-21T16:52:19.972208Z",
          "iopub.execute_input": "2023-03-21T16:52:19.972660Z",
          "iopub.status.idle": "2023-03-21T16:52:19.987305Z",
          "shell.execute_reply.started": "2023-03-21T16:52:19.972616Z",
          "shell.execute_reply": "2023-03-21T16:52:19.986264Z"
        },
        "trusted": true,
        "id": "K0m9oiHekEQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(path, num_epochs, model, optimizer, scheduler):\n",
        "    '''Save on GPU'''\n",
        "    data = {\n",
        "        'num_epochs': num_epochs,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None\n",
        "    }\n",
        "    torch.save(data, path)\n",
        "\n",
        "\n",
        "def load_model(path, device, model, optimizer=None, scheduler=None):\n",
        "    '''Load on GPU'''\n",
        "    data = torch.load(path)\n",
        "    model.load_state_dict(data['model_state_dict'])\n",
        "    model.to(device)\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(data['optimizer_state_dict'])\n",
        "    if scheduler is not None:\n",
        "        scheduler.load_state_dict(data['scheduler_state_dict'])\n",
        "    return data['num_epochs']\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def inference(model, texts, device):\n",
        "    model.eval()\n",
        "    # tranform src texts to tensor\n",
        "    tokenized_texts = []\n",
        "    for text in texts:\n",
        "        tokens = [vocab_de[word] if word in vocab_de else vocab_de['<unk>'] \n",
        "                  for word in text.split()]\n",
        "        tokenized_texts += [[vocab_de['<bos>']] + tokens + [vocab_de['<eos>']]]\n",
        "    \n",
        "    src = torch.full((len(tokenized_texts), max_length), \n",
        "                      vocab_de['<pad>'], dtype=torch.long).to(device)\n",
        "    for i, tokens in enumerate(tokenized_texts):\n",
        "        src[i, :len(tokens)] = torch.tensor(tokens).to(device)\n",
        "    src_length = torch.min(torch.sum(src==vocab_de['<pad>'], dim=-1))\n",
        "    src = src[:, :-src_length]\n",
        "\n",
        "    # make inference\n",
        "    tgt = torch.tensor([vocab_en['<bos>']] * len(texts), \n",
        "                       dtype=torch.long).unsqueeze(-1).to(device)\n",
        "    for _ in range(max_length - 1):\n",
        "        src_mask = None\n",
        "        tgt_mask = generate_square_subsequent_mask(tgt.shape[1]).to(device)\n",
        "        src_padding_mask = (src == vocab_de['<pad>'])\n",
        "        tgt_padding_mask = (tgt == vocab_en['<pad>'])\n",
        "        pred = model(src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "    \n",
        "        new_tokens = torch.argmax(pred[:, -1], dim=1)\n",
        "        tgt = torch.cat((tgt, new_tokens.unsqueeze(-1)), dim=1)\n",
        "\n",
        "    # transform tgt tensor to texts\n",
        "    res = []\n",
        "    for i in range(len(texts)):\n",
        "        pred = list(tgt[i, 1:])\n",
        "        if vocab_en['<eos>'] in pred:\n",
        "            pred = pred[:pred.index(vocab_en['<eos>'])]\n",
        "        pred = pred[:texts[i].count(' ') + 6]   # cut translations to adequate length!\n",
        "        res += [' '.join(vocab_en.lookup_tokens(pred))]\n",
        "    return res\n",
        "\n",
        "\n",
        "def compute_val_bleu(val_predictions_path):\n",
        "    cmd = f'cat {val_predictions_path} | sacrebleu {VAL_EN_PATH}  --tokenize none --width 2 -b'\n",
        "    return float(os.popen(cmd).read())\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def make_predictions(src_path, model, device, tgt_path):\n",
        "    model.eval()\n",
        "    with open(src_path, 'r') as src_file:\n",
        "        src = src_file.readlines()\n",
        "    \n",
        "    predictions = []\n",
        "    for i in range(0, len(src), batch_size):\n",
        "        texts = [text.strip() for text in src[i:i+batch_size]]\n",
        "        predictions += inference(model, texts, device)\n",
        "    \n",
        "    with open(tgt_path, 'w') as tgt_file:\n",
        "        for pred in predictions:\n",
        "            tgt_file.write(pred + '\\n')\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, loader, device):\n",
        "    test_loss = 0.0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for src, target in loader:\n",
        "            src_length = torch.min(torch.sum(src==vocab_de['<pad>'], dim=-1))\n",
        "            src = src[:, :-src_length].to(device)\n",
        "            target_length = torch.min(torch.sum(target==vocab_en['<pad>'], dim=-1))\n",
        "            target = target[:, :-target_length].to(device)\n",
        "\n",
        "            tgt_input = target[:,:-1]\n",
        "            src_mask = None\n",
        "            tgt_mask = generate_square_subsequent_mask(tgt_input.shape[1]).to(device)\n",
        "            src_padding_mask = (src == vocab_de['<pad>'])\n",
        "            tgt_padding_mask = (tgt_input == vocab_en['<pad>'])\n",
        "            pred = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "            loss = F.cross_entropy(pred.reshape(-1, pred.shape[-1]), \n",
        "                                   target[:,1:].reshape(-1), ignore_index=vocab_en['<pad>'])\n",
        "\n",
        "            test_loss += loss.item() * target.shape[0]\n",
        "    \n",
        "    val_predictions_path = WORKING_DIR + 'val_preds.txt'\n",
        "    make_predictions(VAL_DE_PATH, model, device, val_predictions_path)\n",
        "    val_bleu = compute_val_bleu(val_predictions_path)\n",
        "    \n",
        "    return test_loss / len(loader.dataset), val_bleu\n",
        "\n",
        "\n",
        "def train_epoch(model, optimizer, train_loader, device):\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for src, target in train_loader:\n",
        "        src_length = torch.min(torch.sum(src==vocab_de['<pad>'], dim=-1))\n",
        "        src = src[:, :-src_length].to(device)\n",
        "        target_length = torch.min(torch.sum(target==vocab_en['<pad>'], dim=-1))\n",
        "        target = target[:, :-target_length].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        tgt_input = target[:,:-1]\n",
        "        src_mask = None\n",
        "        tgt_mask = generate_square_subsequent_mask(tgt_input.shape[1]).to(device)\n",
        "        src_padding_mask = (src == vocab_de['<pad>'])\n",
        "        tgt_padding_mask = (tgt_input == vocab_en['<pad>'])\n",
        "        pred = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        \n",
        "        loss = F.cross_entropy(pred.reshape(-1, pred.shape[-1]), \n",
        "                               target[:,1:].reshape(-1), \n",
        "                               ignore_index=vocab_en['<pad>'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * target.shape[0]\n",
        "\n",
        "    return train_loss / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "def train_with_wandb(model, optimizer, n_epochs, train_loader, val_loader, device,\n",
        "                     wandb_init_data, scheduler=None, verbose=False):\n",
        "    train_loss_log, val_loss_log, val_bleu_log = [], [], []\n",
        "\n",
        "    with wandb.init(**wandb_init_data) as run:\n",
        "        for epoch in range(n_epochs):\n",
        "            start_epoch = dt.datetime.now()\n",
        "            train_loss = train_epoch(model, optimizer, train_loader, device)\n",
        "            print('Train epoch finished:', dt.datetime.now() - start_epoch)\n",
        "            val_loss, val_bleu = test(model, val_loader, device)\n",
        "            print('Val epoch finished:', dt.datetime.now() - start_epoch)\n",
        "\n",
        "            wandb.log({\"loss/train\": train_loss, \"loss/val\": val_loss, 'bleu/val': val_bleu})\n",
        "\n",
        "            train_loss_log.append(train_loss)\n",
        "            val_loss_log.append(val_loss)\n",
        "            val_bleu_log.append(val_bleu)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch}\\n train loss: {train_loss}\\n val loss: {val_loss}\\n\")\n",
        "\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "            \n",
        "            if epoch % 3 == 2 or epoch == n_epochs - 1:\n",
        "                save_model(WORKING_DIR + f'{epoch + 1}epochs.pt', \n",
        "                           epoch + 1, model, optimizer, scheduler)\n",
        "\n",
        "    return train_loss_log, val_loss_log, val_bleu_log"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-21T16:52:19.989846Z",
          "iopub.execute_input": "2023-03-21T16:52:19.990571Z",
          "iopub.status.idle": "2023-03-21T16:52:20.025458Z",
          "shell.execute_reply.started": "2023-03-21T16:52:19.990522Z",
          "shell.execute_reply": "2023-03-21T16:52:20.024376Z"
        },
        "trusted": true,
        "id": "ax30vKM9kEQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "src_vocab_size = len(vocab_de)\n",
        "tgt_vocab_size = len(vocab_en)\n",
        "embed_dim = 512\n",
        "fc_dim = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "\n",
        "model = MyTransformer(src_vocab_size, tgt_vocab_size, max_length, embed_dim, fc_dim,\n",
        "                      num_heads, num_encoder_layers, num_decoder_layers)\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = None\n",
        "\n",
        "num_epochs = 15\n",
        "\n",
        "wandb_init_data = {\n",
        "    'project': 'bhw2',\n",
        "    'name': 'run',\n",
        "    'config': {\n",
        "        'model': 'nn.Transformer',\n",
        "        'optimizer': optimizer,\n",
        "        'scheduler': scheduler,\n",
        "\n",
        "        'dataset': 'bhw2',\n",
        "        'num_epochs': num_epochs,\n",
        "        'train_loader_batch_size': batch_size,\n",
        "        'dataloader_num_workers': dataloader_num_workers,\n",
        "        'script': _ih[-1]\n",
        "    }\n",
        "}\n",
        "\n",
        "print(sum(param.numel() for param in model.parameters()))\n",
        "train_with_wandb(model, optimizer, num_epochs, train_loader, val_loader, device,\n",
        "                 wandb_init_data, scheduler=scheduler, verbose=True)\n",
        "\n",
        "make_predictions(TEST_DE_PATH, model, device, TEST_EN_PATH)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-21T17:45:54.024538Z",
          "iopub.execute_input": "2023-03-21T17:45:54.025497Z",
          "iopub.status.idle": "2023-03-21T19:26:22.767440Z",
          "shell.execute_reply.started": "2023-03-21T17:45:54.025445Z",
          "shell.execute_reply": "2023-03-21T19:26:22.766174Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "5011ebce75b64b80923c6df34d1ad747"
          ]
        },
        "id": "dS5htc0rkEQ2",
        "outputId": "e27fca33-07c1-4019-ae60-d1fcd00e6f7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "35679896\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.14.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.13.10"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230321_174554-f0ugflgx</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/nik-fedorov/bhw2/runs/f0ugflgx' target=\"_blank\">run</a></strong> to <a href='https://wandb.ai/nik-fedorov/bhw2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/nik-fedorov/bhw2' target=\"_blank\">https://wandb.ai/nik-fedorov/bhw2</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/nik-fedorov/bhw2/runs/f0ugflgx' target=\"_blank\">https://wandb.ai/nik-fedorov/bhw2/runs/f0ugflgx</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Train epoch finished: 0:05:31.598997\nVal epoch finished: 0:06:27.303232\nEpoch 0\n train loss: 4.610298933411005\n val loss: 3.530614066075602\n\nTrain epoch finished: 0:05:33.770618\nVal epoch finished: 0:06:29.318607\nEpoch 1\n train loss: 3.4328198841500845\n val loss: 2.890442795008723\n\nTrain epoch finished: 0:05:34.184909\nVal epoch finished: 0:06:29.658893\nEpoch 2\n train loss: 2.9325144423973115\n val loss: 2.5375775609006745\n\nTrain epoch finished: 0:05:33.551689\nVal epoch finished: 0:06:28.957092\nEpoch 3\n train loss: 2.629366356388909\n val loss: 2.3520810149504254\n\nTrain epoch finished: 0:05:33.692487\nVal epoch finished: 0:06:29.131114\nEpoch 4\n train loss: 2.429152554070831\n val loss: 2.2385110356986644\n\nTrain epoch finished: 0:05:33.771188\nVal epoch finished: 0:06:29.178708\nEpoch 5\n train loss: 2.2861809334717096\n val loss: 2.1725916944701096\n\nTrain epoch finished: 0:05:32.669568\nVal epoch finished: 0:06:28.175332\nEpoch 6\n train loss: 2.176872739050018\n val loss: 2.120466211988283\n\nTrain epoch finished: 0:05:33.325259\nVal epoch finished: 0:06:28.709201\nEpoch 7\n train loss: 2.0881399612162923\n val loss: 2.089392102030663\n\nTrain epoch finished: 0:05:31.049442\nVal epoch finished: 0:06:26.364465\nEpoch 8\n train loss: 2.0148702632771505\n val loss: 2.074936067356783\n\nTrain epoch finished: 0:05:33.053118\nVal epoch finished: 0:06:28.482091\nEpoch 9\n train loss: 1.9504630726560488\n val loss: 2.0508444512349837\n\nTrain epoch finished: 0:05:32.307812\nVal epoch finished: 0:06:27.749111\nEpoch 10\n train loss: 1.893426237000327\n val loss: 2.0417541461348776\n\nTrain epoch finished: 0:05:32.339774\nVal epoch finished: 0:06:27.707822\nEpoch 11\n train loss: 1.8415219058128391\n val loss: 2.0274806235431417\n\nTrain epoch finished: 0:05:32.441513\nVal epoch finished: 0:06:27.757343\nEpoch 12\n train loss: 1.793392919614834\n val loss: 2.024750926673533\n\nTrain epoch finished: 0:05:32.626635\nVal epoch finished: 0:06:28.083830\nEpoch 13\n train loss: 1.7486876230998272\n val loss: 2.031388214587198\n\nTrain epoch finished: 0:05:33.143425\nVal epoch finished: 0:06:28.549805\nEpoch 14\n train loss: 1.7064304158080517\n val loss: 2.018933027801359\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5011ebce75b64b80923c6df34d1ad747"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>bleu/val</td><td>▁▄▅▆▇▇▇████████</td></tr><tr><td>loss/train</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>loss/val</td><td>█▅▃▃▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>bleu/val</td><td>30.74</td></tr><tr><td>loss/train</td><td>1.70643</td></tr><tr><td>loss/val</td><td>2.01893</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">run</strong> at: <a href='https://wandb.ai/nik-fedorov/bhw2/runs/f0ugflgx' target=\"_blank\">https://wandb.ai/nik-fedorov/bhw2/runs/f0ugflgx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230321_174554-f0ugflgx/logs</code>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading model"
      ],
      "metadata": {
        "id": "vTUjJ6cYkEQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "src_vocab_size = len(vocab_de)\n",
        "tgt_vocab_size = len(vocab_en)\n",
        "embed_dim = 512\n",
        "fc_dim = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "\n",
        "model = MyTransformer(src_vocab_size, tgt_vocab_size, max_length, embed_dim, fc_dim,\n",
        "                      num_heads, num_encoder_layers, num_decoder_layers)\n",
        "optimizer = None\n",
        "scheduler = None\n",
        "\n",
        "load_model(WORKING_DIR + '9epochs.pt', device, model, optimizer, scheduler)\n",
        "make_predictions(TEST_DE_PATH, model, device, TEST_EN_PATH)"
      ],
      "metadata": {
        "trusted": true,
        "id": "PEZbfelckEQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Postprocessing"
      ],
      "metadata": {
        "id": "kD4f6Hj-kEQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_file(tgt_path):\n",
        "    with open(tgt_path, 'r') as f:\n",
        "        preds = f.readlines()\n",
        "    \n",
        "    preds = [' '.join(text.replace('<unk>', '') \\\n",
        "                          .replace('<pad>', '') \\\n",
        "                          .replace('<bos>', '') \\\n",
        "                          .replace('<eos>', '').strip().split()) for text in preds]\n",
        "\n",
        "    dedup_preds = []\n",
        "    for text in preds:\n",
        "        tokens = text.split()\n",
        "        dedup_tokens = [tokens[0]] + [tokens[i] for i in range(1, len(tokens)) \n",
        "                                      if tokens[i] != tokens[i - 1]]\n",
        "        dedup_preds.append(' '.join(dedup_tokens) + '\\n')\n",
        "    \n",
        "    with open(WORKING_DIR + 'postprocessed.txt', 'w') as f:\n",
        "        f.writelines(dedup_preds)\n",
        "\n",
        "\n",
        "postprocess_file(WORKING_DIR + 'test1.de-en.en')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-21T19:26:44.699377Z",
          "iopub.execute_input": "2023-03-21T19:26:44.700112Z",
          "iopub.status.idle": "2023-03-21T19:26:44.731721Z",
          "shell.execute_reply.started": "2023-03-21T19:26:44.700067Z",
          "shell.execute_reply": "2023-03-21T19:26:44.730767Z"
        },
        "trusted": true,
        "id": "VYI9fqeXkEQ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}