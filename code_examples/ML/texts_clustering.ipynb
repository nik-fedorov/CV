{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "pe1SQJE_X1t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('Otchet_Za_3_Mes (1).xlsx', sheet_name='Сообщения', skiprows=13)\n",
        "df = df[df['Тип'] == 'Комментарий']"
      ],
      "metadata": {
        "id": "xvsa2EnL6hsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing"
      ],
      "metadata": {
        "id": "gHZRJymQGR5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "id": "UgkDBdojGjZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pymorphy2\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9YEWb-ZIVxU",
        "outputId": "f7234c02-8a32-48b8-c26e-86319379790e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"russian\"))\n",
        "lemmatizer = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = list(filter(str.isalpha, word_tokenize(str(text).lower(), language=\"russian\")))\n",
        "    text = list(lemmatizer.parse(word)[0].normal_form for word in text)\n",
        "    text = list(word for word in text if word not in stop_words)\n",
        "    return ' '.join(text)"
      ],
      "metadata": {
        "id": "bmOIGFu6LGIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess('Я люблю играть с друзьями!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zyDbWpIErtti",
        "outputId": "cf9fdef5-c1e3-4e6d-ff7f-0d9d56d0dbda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'любить играть друг'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['Сообщение'].dropna().apply(lambda row: preprocess(row))\n",
        "texts = texts[texts != '']"
      ],
      "metadata": {
        "id": "-anRZWTqMfJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving and loading preprocessed texts"
      ],
      "metadata": {
        "id": "DEbtH6HGdyOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts.to_csv('preprocessed.csv')  # сохранить тексты в файлик\n",
        "# НЕ ЗАБУДЬ СКАЧАТЬ ФАЙЛИК СЕБЕ НА КОМП, ИНАЧЕ ОН МОЖЕТ УДАЛИТЬСЯ ИЗ КОЛАБА ЧЕРЕЗ ВРЕМЯ"
      ],
      "metadata": {
        "id": "fLX-86xNatDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = pd.read_csv('preprocessed.csv', index_col=0)['Сообщение']  # загрузить тексты из указанного файлика"
      ],
      "metadata": {
        "id": "tatu-Z4dez4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tf-Idf and dimensionality reduction"
      ],
      "metadata": {
        "id": "z20LCihtXqAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf_idf = TfidfVectorizer().fit_transform(texts)\n",
        "X = TruncatedSVD(100).fit_transform(tf_idf)"
      ],
      "metadata": {
        "id": "MIR-us4cT5Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering"
      ],
      "metadata": {
        "id": "NH925ZvXYFzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans, SpectralClustering\n",
        "\n",
        "# clusters = DBSCAN(eps=0.1, metric='cosine').fit_predict(X)\n",
        "clusters = AgglomerativeClustering(n_clusters=50, metric='cosine', linkage='average').fit_predict(X)"
      ],
      "metadata": {
        "id": "wsc-AH3LXfJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(clusters)  # посмотреть номера кластеров и их количество"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOpcJIyNl1R8",
        "outputId": "943c7293-1275-44e5-c127-94f000269a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(clusters == 0).sum()  # количество текстов в 6 кластере"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKLGvYA8mTHe",
        "outputId": "84b9ec1a-fc55-45e9-dfa8-7af3f3a41ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "242"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[clusters == 55]  # посмотреть все тексты в 55 кластере "
      ],
      "metadata": {
        "id": "RIPpDzEyoQ9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[texts[clusters == 4].index]['Сообщение']  # посмотреть сырые тексты, соответствующие 4-му кластеру"
      ],
      "metadata": {
        "id": "qm1HOBkegh75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Удаление кластера"
      ],
      "metadata": {
        "id": "Yvni6SjXysB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Осторожно! Стирает часть данных. Запускайте код с умом!\n",
        "texts = texts[clusters != 4]  # удаляет все тексты из 4-го кластера\n",
        "# Теперь сохрани оставшиеся тексты в файлик (см. раздел saving preprocessed texts)\n",
        "texts = texts[-pd.Series(clusters).isin([2, 3, 5])]"
      ],
      "metadata": {
        "id": "jxiRlyGAw-XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Удаление стоп-слов"
      ],
      "metadata": {
        "id": "ZwPFo4jfyv0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# удалить сразу все стоп слова из файлика\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"russian\"))\n",
        "with open('stopwords.txt', 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        stop_words.add(line.strip())\n",
        "texts = texts.apply(lambda t: ' '.join(word for word in str(t).split() if word not in stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vorhlxy3y0aN",
        "outputId": "ac10bb1c-0259-42fd-c1ee-9660f9f80da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# удалить из текстов очередное стоп-слово\n",
        "texts = texts.apply(lambda t: ' '.join(word for word in str(t).split() if word != 'новоестопслово'))"
      ],
      "metadata": {
        "id": "f0YYNQgrh598"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# удаляем пустые тексты\n",
        "texts = texts[texts != '']"
      ],
      "metadata": {
        "id": "oOLE_ofpy2k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Выгрузить эксельку"
      ],
      "metadata": {
        "id": "bkyiQhqNy3Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clusters_to_excel(raw_texts, preprocessed_texts, clusters):\n",
        "    iterables = [list(np.unique(clusters)), ['Сырые тексты', 'Обработанные тексты']]\n",
        "    index = pd.MultiIndex.from_product(iterables, names=['Номер кластера', 'Представление текста'])\n",
        "    biggest_cluster_size = pd.Series(clusters).value_counts().sort_values().iloc[-1]\n",
        "    df = pd.DataFrame(index=pd.Index(range(biggest_cluster_size)), columns=index)\n",
        "    \n",
        "    for clust in np.unique(clusters):\n",
        "        df[clust, 'Сырые тексты'] = raw_texts[clusters == clust].reset_index(drop=True)\n",
        "        df[clust, 'Обработанные тексты'] = preprocessed_texts[clusters == clust].reset_index(drop=True)\n",
        "    \n",
        "    df.to_excel('Кластеры.xlsx')"
      ],
      "metadata": {
        "id": "WFUeZub8J7IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusters_to_excel(df.loc[texts.index]['Сообщение'], texts, clusters)"
      ],
      "metadata": {
        "id": "aZxZKtp3RxPT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}