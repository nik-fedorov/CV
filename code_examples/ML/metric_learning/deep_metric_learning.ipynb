{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modifications of triplet loss for deep metric learning tasks\n",
        "\n",
        "This notebook contains code for experiments with CUB-200-2011 dataset. \n",
        "\n",
        "Requirements: pandas, numpy, torch, wandb, timm, open-metric-learning"
      ],
      "metadata": {
        "id": "vD3vQAaPR_qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-18T19:39:30.605739Z",
          "iopub.execute_input": "2023-05-18T19:39:30.606069Z",
          "iopub.status.idle": "2023-05-18T19:39:33.440285Z",
          "shell.execute_reply.started": "2023-05-18T19:39:30.606036Z",
          "shell.execute_reply": "2023-05-18T19:39:33.438964Z"
        },
        "trusted": true,
        "id": "uBUtbEYAR_qc",
        "outputId": "f719e1cc-a773-45c1-9b7b-a320c010884f"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 1,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/OML-Team/open-metric-learning.git\n",
        "!pip install wandb\n",
        "!wandb login \"YOUR API KEY\"\n",
        "\n",
        "!wget \"https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz\"\n",
        "!tar -zxvf CUB_200_2011.tgz\n",
        "\n",
        "!wget \"https://raw.githubusercontent.com/OML-Team/open-metric-learning/main/pipelines/datasets_converters/convert_cub.py\"\n",
        "!python convert_cub.py --dataset_root=/kaggle/working/CUB_200_2011 --no_bboxes"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "scrolled": true,
        "trusted": true,
        "id": "e15NX1dvR_qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from oml.datasets.base import DatasetWithLabels\n",
        "from oml.inference.flat import inference_on_images\n",
        "from oml.losses.triplet import TripletLossWithMiner\n",
        "from oml.miners.cross_batch import TripletMinerWithMemory\n",
        "from oml.miners.inbatch_hard_cluster import HardClusterMiner\n",
        "from oml.miners.inbatch_hard_tri import HardTripletsMiner\n",
        "from oml.miners.inbatch_nhard_tri import NHardTripletsMiner\n",
        "from oml.miners.inbatch_all_tri import AllTripletsMiner\n",
        "from oml.models.vit.vit import ViTExtractor\n",
        "from oml.samplers.balance import BalanceSampler\n",
        "from oml.transforms.images.albumentations import (\n",
        "    get_augs_albu,\n",
        "    get_normalisation_resize_albu\n",
        ")\n",
        "from oml.transforms.images.torchvision import (\n",
        "    get_augs_hypvit,\n",
        "    get_normalisation_resize_hypvit\n",
        ")\n",
        "\n",
        "import wandb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-18T19:41:43.035838Z",
          "iopub.execute_input": "2023-05-18T19:41:43.036222Z",
          "iopub.status.idle": "2023-05-18T19:41:46.204469Z",
          "shell.execute_reply.started": "2023-05-18T19:41:43.036176Z",
          "shell.execute_reply": "2023-05-18T19:41:46.203134Z"
        },
        "trusted": true,
        "id": "dOAk1eyER_qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from oml.functional.metrics import (\n",
        "    calc_gt_mask,\n",
        "    calc_mask_to_ignore,\n",
        "    calc_retrieval_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(dist_mat, labels, is_query, is_gallery, **metrics):\n",
        "    mask_gt = calc_gt_mask(labels=labels, is_query=is_query, is_gallery=is_gallery)\n",
        "    mask_to_ignore = calc_mask_to_ignore(is_query=is_query, is_gallery=is_gallery)\n",
        "    return calc_retrieval_metrics(dist_mat, mask_gt, mask_to_ignore, **metrics)\n",
        "\n",
        "\n",
        "def transform_metrics_for_wandb_logging(metrics_value):\n",
        "    res = {}\n",
        "    for metric_name in metrics_value:\n",
        "        for k in metrics_value[metric_name]:\n",
        "            res[metric_name + '/' + str(k)] = metrics_value[metric_name][k].item()\n",
        "    return res\n",
        "\n",
        "\n",
        "def save_model(path, num_epochs, model, optimizer, scheduler=None):\n",
        "    '''Save on GPU'''\n",
        "    data = {\n",
        "        'num_epochs': num_epochs,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None\n",
        "    }\n",
        "    torch.save(data, path)\n",
        "\n",
        "\n",
        "def load_model(path, device, model, optimizer=None, scheduler=None):\n",
        "    '''Load on GPU'''\n",
        "    data = torch.load(path)\n",
        "    model.load_state_dict(data['model_state_dict'])\n",
        "    model.to(device)\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(data['optimizer_state_dict'])\n",
        "    if scheduler is not None:\n",
        "        scheduler.load_state_dict(data['scheduler_state_dict'])\n",
        "    return data['num_epochs']\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def inference(model, valid_loader, device):\n",
        "    embeds, labels = [], []\n",
        "    for batch in valid_loader:\n",
        "        # embeds += [F.normalize(model.body(batch['input_tensors'].to(device)), p=2, dim=1)]\n",
        "        embeds += [model(batch['input_tensors'].to(device))]\n",
        "        labels += [batch['labels']]\n",
        "    return torch.cat(embeds, dim=0).cpu(), torch.cat(labels, dim=0).cpu()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def track_additional_valid_metrics(embeds, labels, dist_mat):\n",
        "    additionel_metrics = {}\n",
        "    \n",
        "    class_centers, class_sizes = [], []\n",
        "    for label, count in zip(*torch.unique(labels, return_counts=True)):\n",
        "        class_embeds = embeds[labels == label]\n",
        "        class_center = torch.mean(class_embeds, dim=0)\n",
        "        class_centers += [class_center]\n",
        "        class_variance = torch.sum((class_embeds - class_center.unsqueeze(0)) ** 2) / count\n",
        "        class_sizes += [torch.sqrt(class_variance)]\n",
        "    \n",
        "    class_sizes = torch.tensor(class_sizes)\n",
        "    class_centers = torch.stack(class_centers, dim=0)\n",
        "    class_centers_dist_mat = torch.cdist(class_centers, class_centers, p=2)\n",
        "    n_classes = class_centers.shape[0]\n",
        "    \n",
        "    additionel_metrics['additional/class_sizes/min'] = torch.min(class_sizes)\n",
        "    additionel_metrics['additional/class_sizes/max'] = torch.max(class_sizes)\n",
        "    additionel_metrics['additional/class_sizes/mean'] = torch.mean(class_sizes)\n",
        "    \n",
        "    additionel_metrics['additional/inter_class_dist/min'] = \\\n",
        "        torch.min(class_centers_dist_mat[class_centers_dist_mat > 0])\n",
        "    additionel_metrics['additional/inter_class_dist/max'] = torch.max(class_centers_dist_mat)\n",
        "    additionel_metrics['additional/inter_class_dist/mean'] = \\\n",
        "        torch.sum(class_centers_dist_mat) / float(n_classes) / (n_classes - 1)\n",
        "    \n",
        "    return additionel_metrics\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validation(model, valid_loader, metrics, device):\n",
        "    model.eval()\n",
        "    embeds, labels = inference(model, valid_loader, device)\n",
        "    print(f'Inference finished: {dt.datetime.now()}')\n",
        "\n",
        "    dist_mat = torch.cdist(embeds, embeds, p=2)\n",
        "    mask = torch.ones(len(embeds))\n",
        "    metrics_value = compute_metrics(dist_mat, labels, mask, mask, **metrics)\n",
        "    wandb_metrics_value = transform_metrics_for_wandb_logging(metrics_value)\n",
        "    wandb_metrics_value.update(track_additional_valid_metrics(embeds, labels, dist_mat))\n",
        "    print(wandb_metrics_value, end='\\n\\n')\n",
        "    \n",
        "    return wandb_metrics_value\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def add_train_metrics(wandb_metrics_value, model, train_loader, metrics, device):\n",
        "    train_metrics_value = validation(model, train_loader, metrics, device)\n",
        "    for metric, value in train_metrics_value.items():\n",
        "        wandb_metrics_value['train/' + metric] = value\n",
        "\n",
        "\n",
        "def curve_relu(x, gamma):\n",
        "    gamma = float(gamma)\n",
        "    assert gamma >= 1.0\n",
        "    \n",
        "    res = torch.clone(x)\n",
        "    mask = (res < 0)\n",
        "    res[mask] = 0.0\n",
        "    res[torch.logical_not(mask)] **= gamma\n",
        "    return res\n",
        "\n",
        "\n",
        "def relu_threshold(x, t):\n",
        "    t = float(t)\n",
        "    assert t > 0\n",
        "    \n",
        "    res = torch.clone(x)\n",
        "    res[res < 0] = 0.0\n",
        "    res[res > t] = t\n",
        "    return res"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-18T19:41:46.206206Z",
          "iopub.execute_input": "2023-05-18T19:41:46.207191Z",
          "iopub.status.idle": "2023-05-18T19:41:46.236806Z",
          "shell.execute_reply.started": "2023-05-18T19:41:46.207144Z",
          "shell.execute_reply": "2023-05-18T19:41:46.235761Z"
        },
        "trusted": true,
        "id": "Cr4Uqu25R_qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze(model):\n",
        "    def fr(m):\n",
        "        for param in m.parameters():\n",
        "            param.requires_grad = False\n",
        "    fr(model.patch_embed)\n",
        "    fr(model.pos_drop)\n",
        "\n",
        "\n",
        "def rm_head(m):\n",
        "    names = set(x[0] for x in m.named_children())\n",
        "    target = {\"head\", \"fc\", \"head_dist\"}\n",
        "    for x in names & target:\n",
        "        m.add_module(x, nn.Identity())\n",
        "\n",
        "\n",
        "class NormLayer(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return F.normalize(x, p=2, dim=1)\n",
        "\n",
        "\n",
        "class Extractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.body = timm.create_model('vit_small_patch16_224', pretrained=True)\n",
        "        # self.body = torch.hub.load(\"facebookresearch/dino:main\", 'dino_vits16')\n",
        "        # self.body = ViTExtractor('vits8_dino', arch='vits8', normalise_features=False)\n",
        "        freeze(self.body)   # freeze MLPs for patch embeds\n",
        "        \n",
        "#         self.head = nn.Sequential(nn.Linear(384, 384), NormLayer())\n",
        "#         nn.init.constant_(self.head[0].bias.data, 0)\n",
        "#         nn.init.orthogonal_(self.head[0].weight.data)\n",
        "        rm_head(self.body)\n",
        "        # self.head = nn.Identity()\n",
        "        self.head = NormLayer()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.head(self.body(x))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-18T19:41:46.240055Z",
          "iopub.execute_input": "2023-05-18T19:41:46.240439Z",
          "iopub.status.idle": "2023-05-18T19:41:48.280920Z",
          "shell.execute_reply.started": "2023-05-18T19:41:46.240398Z",
          "shell.execute_reply": "2023-05-18T19:41:48.279808Z"
        },
        "trusted": true,
        "id": "J1d0Sll2R_qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Triplet loss implementation from OML library"
      ],
      "metadata": {
        "id": "M0pRrlQdR_qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from oml.functional.losses import get_reduced\n",
        "from oml.interfaces.miners import labels2list\n",
        "from oml.utils.misc_torch import elementwise_dist\n",
        "\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "\n",
        "class MyTripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Class, which combines classical `TripletMarginLoss` and `SoftTripletLoss`.\n",
        "    The idea of `SoftTripletLoss` is the following:\n",
        "    instead of using the classical formula\n",
        "    ``loss = relu(margin + positive_distance - negative_distance)``\n",
        "    we use\n",
        "    ``loss = log1p(exp(positive_distance - negative_distance))``.\n",
        "    It may help to solve the often problem when `TripletMarginLoss` converges to it's\n",
        "    margin value (also known as `dimension collapse`).\n",
        "    \"\"\"\n",
        "\n",
        "    criterion_name = \"triplet\"  # for better logging\n",
        "\n",
        "    def __init__(self, margin: Optional[float], reduction: str = \"mean\", need_logs: bool = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            margin: Margin value, set ``None`` to use `SoftTripletLoss`\n",
        "            reduction: ``mean``, ``sum`` or ``none``\n",
        "            need_logs: Set ``True`` if you want to store logs\n",
        "        \"\"\"\n",
        "        assert reduction in (\"mean\", \"sum\", \"none\")\n",
        "        # assert (margin is None) or (margin > 0)\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.margin = margin\n",
        "        self.reduction = reduction\n",
        "        self.need_logs = need_logs\n",
        "        self.last_logs: Dict[str, float] = {}\n",
        "        \n",
        "        self.log_dap, self.log_dan, self.log_dpn = [], [], []\n",
        "\n",
        "    def forward(self, anchor: Tensor, positive: Tensor, negative: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            anchor: Anchor features with the shape of ``(batch_size, feat)``\n",
        "            positive: Positive features with the shape of ``(batch_size, feat)``\n",
        "            negative: Negative features with the shape of ``(batch_size, feat)``\n",
        "        Returns:\n",
        "            Loss value\n",
        "        \"\"\"\n",
        "        assert anchor.shape == positive.shape == negative.shape\n",
        "\n",
        "        positive_dist = elementwise_dist(x1=anchor, x2=positive, p=2)\n",
        "        negative_dist = elementwise_dist(x1=anchor, x2=negative, p=2)\n",
        "        pos_neg_dist = elementwise_dist(x1=positive, x2=negative, p=2)\n",
        "        \n",
        "        self.log_dap.append(positive_dist)\n",
        "        self.log_dan.append(negative_dist)\n",
        "        self.log_dpn.append(pos_neg_dist)\n",
        "\n",
        "        if self.margin is None:\n",
        "            # here is the soft version of TripletLoss without margin\n",
        "            loss = torch.log1p(torch.exp(positive_dist - negative_dist))\n",
        "        else:\n",
        "            # loss = torch.relu(self.margin + positive_dist - negative_dist)\n",
        "            loss = curve_relu(self.margin + positive_dist - negative_dist, gamma=2.0)\n",
        "            # loss = relu_threshold(self.margin + positive_dist - negative_dist, t=self.margin + 0.45)\n",
        "\n",
        "        if self.need_logs:\n",
        "            self.last_logs = {\n",
        "                \"active_tri\": float((loss.clone().detach() > 0).float().mean()),\n",
        "                \"pos_dist\": float(positive_dist.clone().detach().mean().item()),\n",
        "                \"neg_dist\": float(negative_dist.clone().detach().mean().item()),\n",
        "            }\n",
        "\n",
        "        loss = get_reduced(loss, reduction=self.reduction)\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def summary(self):\n",
        "        res = {}\n",
        "        dap, dan, dpn = map(torch.cat, [self.log_dap, self.log_dan, self.log_dpn])\n",
        "        diff = dan - dap\n",
        "        for data, name in zip([dap, dan, dpn, diff], \n",
        "                              ['d(a,p)', 'd(a,n)', 'd(p,n)', 'd(a,n)-d(a,p)']):\n",
        "            res[name + '/min'] = torch.min(data).item()\n",
        "            res[name + '/mean'] = torch.mean(data).item()\n",
        "            res[name + '/std'] = torch.std(data).item()\n",
        "            res[name + '/max'] = torch.max(data).item()\n",
        "        self.log_dap, self.log_dan, self.log_dpn = [], [], []\n",
        "        return res\n",
        "\n",
        "\n",
        "class MyTripletLossWithMiner(nn.Module):\n",
        "    def __init__(self, triplet_loss, miner):\n",
        "        super().__init__()\n",
        "        self.tri_loss = triplet_loss\n",
        "        self.miner = miner\n",
        "    \n",
        "    def forward(self, features, labels):\n",
        "        labels_list = labels2list(labels)\n",
        "        anchor, positive, negative = self.miner.sample(features=features, labels=labels_list)\n",
        "        loss = self.tri_loss(anchor=anchor, positive=positive, negative=negative)\n",
        "        return loss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-18T19:41:48.282725Z",
          "iopub.execute_input": "2023-05-18T19:41:48.283146Z",
          "iopub.status.idle": "2023-05-18T19:41:48.357811Z",
          "shell.execute_reply.started": "2023-05-18T19:41:48.283105Z",
          "shell.execute_reply": "2023-05-18T19:41:48.356571Z"
        },
        "trusted": true,
        "id": "lbdNL2yLR_qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_root = '/kaggle/working/CUB_200_2011'\n",
        "num_workers = 2\n",
        "valid_batch_size = 128\n",
        "n_labels = 24\n",
        "n_instances = 4\n",
        "\n",
        "lr = 1e-5\n",
        "wd = 0.01\n",
        "\n",
        "df = pd.read_csv(dataset_root + '/df.csv')\n",
        "# use trainval split as in DML articles\n",
        "df[['is_query', 'is_gallery']] = np.nan\n",
        "df.loc[df['label'] <= 100, 'split'] = 'train'\n",
        "df.loc[df['label'] > 100, 'split'] = 'validation'\n",
        "df.loc[df['label'] > 100, ['is_query', 'is_gallery']] = True\n",
        "\n",
        "df_train = df[df['split'] == 'train']\n",
        "df_valid = df[df['split'] == 'validation']\n",
        "\n",
        "# train_transforms = get_augs_albu(224)\n",
        "# valid_transforms = get_normalisation_resize_albu(224)\n",
        "mean, std = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
        "train_transforms = get_augs_hypvit(224, mean=mean, std=std)\n",
        "valid_transforms = get_normalisation_resize_hypvit(256, mean=mean, std=std)\n",
        "\n",
        "train_dataset = DatasetWithLabels(df_train, transform=train_transforms, dataset_root=dataset_root)\n",
        "train_dataset_metrics = DatasetWithLabels(df_train, transform=valid_transforms, dataset_root=dataset_root)\n",
        "valid_dataset = DatasetWithLabels(df_valid, transform=valid_transforms, dataset_root=dataset_root)\n",
        "\n",
        "sampler = BalanceSampler(train_dataset.get_labels(), n_labels=n_labels, n_instances=n_instances)\n",
        "train_loader = DataLoader(train_dataset, batch_sampler=sampler, num_workers=num_workers)\n",
        "train_loader_metrics = DataLoader(train_dataset_metrics, batch_size=valid_batch_size, num_workers=num_workers)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=valid_batch_size, num_workers=num_workers)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = Extractor().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "scheduler = None\n",
        "# criterion = TripletLossWithMiner(margin=0.15, miner=HardTripletsMiner())\n",
        "criterion = MyTripletLossWithMiner(MyTripletLoss(margin=0.25), \n",
        "                                   NHardTripletsMiner(n_positive=(2, 2), n_negative=(1, 1)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-18T21:11:24.381110Z",
          "iopub.execute_input": "2023-05-18T21:11:24.382212Z",
          "iopub.status.idle": "2023-05-18T21:11:25.413755Z",
          "shell.execute_reply.started": "2023-05-18T21:11:24.382166Z",
          "shell.execute_reply": "2023-05-18T21:11:25.412671Z"
        },
        "trusted": true,
        "id": "jPZikaAPR_qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "rKRCRAANR_qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "n_epochs = 10000\n",
        "valid_period = 5\n",
        "\n",
        "metrics = {\n",
        "    'cmc_top_k': [1],  # to calculate cmc@1\n",
        "    'map_top_k': [5],  # to calculate map@5\n",
        "    'precision_top_k': [],\n",
        "    'fmr_vals': []\n",
        "}\n",
        "\n",
        "wandb_init_data = {\n",
        "    'project': 'TP3',\n",
        "    'name': 'run',\n",
        "    'save_code': True,\n",
        "    'config': {\n",
        "        'model': 'ViT',\n",
        "        'optimizer': optimizer,\n",
        "        'scheduler': scheduler,\n",
        "        'sampler': {\n",
        "            'name': 'balanced',\n",
        "            'n_labels': n_labels,\n",
        "            'n_instances': n_instances\n",
        "        },\n",
        "        \n",
        "        'valid_period': valid_period,\n",
        "\n",
        "        'dataset': 'CUB_200_2011',\n",
        "        'num_epochs': n_epochs,\n",
        "        'dataloader_num_workers': num_workers,\n",
        "        'script': _ih[-1]\n",
        "    }\n",
        "}\n",
        "\n",
        "with wandb.init(**wandb_init_data) as run:\n",
        "    print('Evaluating pre-trained model before training')\n",
        "    wandb_metrics_value = validation(model, valid_loader, metrics, device)\n",
        "    add_train_metrics(wandb_metrics_value, model, train_loader_metrics, metrics, device)\n",
        "    wandb.log(wandb_metrics_value)\n",
        "    best_cmc1 = wandb_metrics_value['cmc/1']\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            embeddings = model(batch['input_tensors'].to(device))\n",
        "            loss = criterion(embeddings, batch['labels'].to(device))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % valid_period == 0:\n",
        "            print(f'{epoch + 1} training epochs finished\\nValidation started: {dt.datetime.now()}')\n",
        "            with torch.inference_mode():\n",
        "                wandb_metrics_value = validation(model, valid_loader, metrics, device)\n",
        "                add_train_metrics(wandb_metrics_value, model, train_loader_metrics, metrics, device)\n",
        "                wandb_metrics_value.update(criterion.tri_loss.summary())\n",
        "                wandb.log(wandb_metrics_value)\n",
        "                \n",
        "                if wandb_metrics_value['cmc/1'] > best_cmc1:\n",
        "                    best_cmc1 = wandb_metrics_value['cmc/1']\n",
        "                    save_model('best.pt', epoch + 1, model, optimizer, scheduler)\n",
        "                    wandb.save('best.pt')\n",
        "                    print(f'\\nNew best CMC@1 {best_cmc1} at {epoch + 1} epoch\\n')"
      ],
      "metadata": {
        "trusted": true,
        "id": "705_ZVcKR_ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading model from wandb and resume training"
      ],
      "metadata": {
        "id": "nZbOfWtlR_ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "best_model = wandb.restore('best.pt', run_path=\"nik-fedorov/TP3/f0ian1ey\")\n",
        "\n",
        "# model = ViTExtractor('vits16_dino', arch='vits16', normalise_features=False).to(device)\n",
        "# model = timm.create_model('vit_small_patch16_224', pretrained=True).to(device)\n",
        "load_model(best_model.name, device, model, optimizer)"
      ],
      "metadata": {
        "trusted": true,
        "id": "qT6eRv1hR_qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "n_epochs = 10000\n",
        "valid_period = 10\n",
        "\n",
        "metrics = {\n",
        "    'cmc_top_k': [1],  # to calculate cmc@1\n",
        "    'map_top_k': [5],  # to calculate map@5\n",
        "    'precision_top_k': [],\n",
        "    'fmr_vals': []\n",
        "}\n",
        "\n",
        "wandb_init_data = {\n",
        "    'project': 'TP3',\n",
        "    'name': 'run',\n",
        "    'save_code': True,\n",
        "    'config': {\n",
        "        'model': 'ViT',\n",
        "        'optimizer': optimizer,\n",
        "        'scheduler': scheduler,\n",
        "        'sampler': {\n",
        "            'name': 'balanced',\n",
        "            'n_labels': n_labels,\n",
        "            'n_instances': n_instances\n",
        "        },\n",
        "        \n",
        "        'valid_period': valid_period,\n",
        "\n",
        "        'dataset': 'CUB_200_2011',\n",
        "        'num_epochs': n_epochs,\n",
        "        'dataloader_num_workers': num_workers,\n",
        "        'script': _ih[-1]\n",
        "    }\n",
        "}\n",
        "\n",
        "with wandb.init(**wandb_init_data) as run:\n",
        "    print('Evaluating pre-trained model before training')\n",
        "    wandb_metrics_value = validation(model, valid_loader, metrics, device)\n",
        "    wandb.log(wandb_metrics_value)\n",
        "    best_cmc1 = wandb_metrics_value['cmc/1']\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            embeddings = model(batch['input_tensors'].to(device))\n",
        "            loss = criterion(embeddings, batch['labels'].to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % valid_period == 0:\n",
        "            print(f'{epoch + 1} training epochs finished\\nValidation started: {dt.datetime.now()}')\n",
        "            with torch.inference_mode():\n",
        "                wandb_metrics_value = validation(model, valid_loader, metrics, device)\n",
        "                wandb.log(wandb_metrics_value)\n",
        "                \n",
        "                if wandb_metrics_value['cmc/1'] > best_cmc1:\n",
        "                    best_cmc1 = wandb_metrics_value['cmc/1']\n",
        "                    save_model('best.pt', epoch + 1, model, optimizer, scheduler)\n",
        "                    wandb.save('best.pt')\n",
        "                    print(f'\\nNew best CMC@1 {best_cmc1} at {epoch + 1} epoch\\n')"
      ],
      "metadata": {
        "trusted": true,
        "id": "xRyJm_SJR_qm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}